---
title: "Lac du Bois STG Habitat Analysis"
format: html
editor: visual
---

### Tile LDB LiDAR data

```{r}
library(lidR)
library(future)

ldb_liDATA <- readLAScatalog("LacDuBois.las")

plot(ldb_liDATA, mapview = TRUE)

opt_output_files(ldb_liDATA) <- "01_retile/{XLEFT}_{YBOTTOM}"

opt_chunk_size(ldb_liDATA) <- 500

#should not use a buffer when tiling, changed buffer to 0 (was 15)
opt_chunk_buffer(ldb_liDATA) <- 0

opt_chunk_alignment(ldb_liDATA) <- c(500, 500)
plot(ldb_liDATA, chunk_pattern = TRUE)

ldb_tiled <- catalog_retile(ldb_liDATA)

plot(ldb_tiled, mapview = TRUE)
```

### Perform ground point classification

```{r}
n_cores <- availableCores() / 2
plan(multisession, workers = n_cores)

opt_output_files(ldb_tiled) <- "02_ground/{XLEFT}_{YBOTTOM}"

# here is where we want to add the buffer (only once we want to run other functions than tiling) 
opt_chunk_buffer(ldb_tiled)<-15

ldb_ground <- classify_ground(ldb_tiled, algorithm = csf(sloop_smooth = TRUE))
```

### Create DEM for Lac du Bois from LiDAR data

```{r}
opt_output_files(ldb_ground) <- ""
dem <- rasterize_terrain(ldb_ground, res = 1, algorithm = tin())
```

### Produce Various Terrain-Based Layers from the DEM

```{r}
library(Rsagacmd)
library(sf)
library(tidyverse)
library(lidR)
library(future)
library(terra)
library(mapview)

dir.create("ta", showWarnings = FALSE)
dem <- writeRaster(dem, "ta/dem.tif", overwrite = TRUE)

#locate where our saga program and provide proper path
saga_path <- "C:\\Users\\cwbut\\OneDrive - Thompson Rivers University\\Documents\\Eco modelling\\Assignment 8 - Big data, SAGA GIS-20240312\\saga-9.3.2_x64\\saga-9.3.2_x64/saga_cmd.exe"

# Create an object (saga) which is a list-like object that contains all of the SAGA GIS functions.
saga <- saga_gis(saga_path, raster_format = "GeoTIFF")

# 1- sink filled DEM:
dem_preproc <- saga$ta_preprocessor$sink_removal(
  dem = dem, dem_preproc = "ta/dem_preproc.tif")
sources(dem_preproc)

# 2- Produce DSM
# I added a DSM, but we don't have to include it in model
# just use CHM in model instead
opt_output_files(ldb_ground) <- ""
dsm <- rasterize_canopy(ldb_ground, res = 1, algorithm = dsmtin())
dsm <- writeRaster(dsm, "ta/dsm.tif", overwrite = TRUE)

plot(dsm)
plot_dtm3d(dsm) # to see it in 3D


# 3- Produce CHM

# Normalize heights: 
opt_chunk_size(ldb_ground) <- 0
opt_output_files(ldb_ground) <- "03_norm/{*}"
ldb_norm <- normalize_height(ldb_ground, algorithm = tin())


# Create CHM
opt_output_files(ldb_norm) <- ""
chm <- rasterize_canopy(ldb_norm, res = 1, algorithm = dsmtin())
chm <- writeRaster(chm, "ta/chm.tif", overwrite = TRUE)

plot(chm)
plot_dtm3d(chm) # to see it in 3D

####### Morphometry layers
# 4- produce slope and aspect layer:
# If want to take a look at all pieces of this function:
View(tidy(saga$ta_morphometry$slope_aspect_curvature))

slope_aspect <- saga$ta_morphometry$slope_aspect_curvature(
  elevation = dem_preproc, slope = "ta/slope.tif", aspect = "ta/aspect.tif", 
  method = 6, unit_slope = "radians", unit_aspect = "radians",
  .all_outputs = FALSE)


# 5- MRVBF/MRRTF
# (Multi-Resolution Valley Bottom Flatness/Multi-Resolution Ridge Top Flatness)
mrvbf_thresh <- mrvbf_threshold(res = res(dem)[1])
mrvbf <- saga$ta_morphometry$multiresolution_index_of_valley_bottom_flatness_mrvbf(
  dem = dem_preproc, mrvbf = "ta/mrvbf.tif", mrrtf = "ta/mrrtf.tif", 
  t_slope = mrvbf_thresh)

# 6- Terrain ruggedness index:
tri <- saga$ta_morphometry$terrain_ruggedness_index_tri(
  dem = dem_preproc, tri = "ta/tri.tif")

####### Hydrology layers

### Following 4 lines (tca, sca, so, and cn) needed to create hydrology layers
### but not needed for modeling

# Total catchment area
tca <- saga$ta_hydrology$flow_accumulation_top_down(
  elevation = dem_preproc, flow = "ta/tca_TEMP.tif", .all_outputs = FALSE)

# Specific catchment area
sca <- saga$ta_hydrology$flow_width_and_specific_catchment_area(
  dem = dem_preproc, tca = tca, sca = "ta/sca_TEMP.tif", .all_outputs = FALSE)

# Strahler stream order
so <- saga$ta_channels$strahler_order(
  dem = dem_preproc, strahler = "ta/strahler_TEMP.tif", .all_outputs = FALSE)

# Channel network
cn <- saga$ta_channels$channel_network(
  elevation = dem_preproc, init_grid = so, init_value = 5,
  chnlntwrk = "ta/cn_TEMP.tif", .all_outputs = FALSE)

# 7- Topographic wetness index
twi <- saga$ta_hydrology$topographic_wetness_index(
  slope = slope_aspect$slope, area = sca, twi = "ta/twi.tif")

# 8 - Overland flow distance
overland_flow <- saga$ta_channels$overland_flow_distance_to_channel_network(
  elevation = dem_preproc, channels = cn, distance = "ta/o_flow.tif", 
  disthorz = "ta/h_flow.tif", distvert = "ta/v_flow.tif", boundary = FALSE, 
  .all_outputs = FALSE)

####### Lighting/visibility layers
# 9-Topographic openness
openness <- saga$ta_lighting$topographic_openness(
  dem = dem_preproc, pos = "ta/openness_pos.tif", neg = "ta/openness_neg.tif")
openness

```

### Extract the raster data where the points are located

```{r}
library(raster)


# Combine all layers into a single object
model_layers <- c(
  dem_preproc, dem, chm, slope_aspect$aspect, mrvbf$mrvbf, mrvbf$mrrtf, tri, twi, overland_flow$distance, openness$pos, openness$neg)
### chm layer produced an error: [rast] extents do not match. This is because I was doing trial runs with the .las file from Assignment 7 and the locations don't overlap.  ldb_norm gave a warning:elements of x are not a spatraster
### I decides to keep ldb_norm out of the model_layers because of this warning.


# Convert .gpkg to a SpatVector object. Lecture 5.
sf_grouse <- st_read("Sharp-tailed grouse.gpkg", stringsAsFactors = TRUE) 
grouse <- vect(sf_grouse) 
mapview(grouse)


# Perform data extraction at each point in each raster layer
ldb_data <- terra::extract(model_layers, grouse, bind = TRUE, na.rm = TRUE) %>% 
  st_as_sf()
mapview(ldb_data)
ldb_data <- na.omit(ldb_data)



#### All the codes are running well, the only issue is that the .las file I used (from A7) doesn't cover the same area as the grouse geopackage. It would be worth running the codes with the proper .las file, but I don't know how to do that without waiting 3 days for it to run.
### In order to run model_layers and ldb_data you will need to remove chm from the list above.
```

### Create a tuned ranger classification probability model using the mlr3 package, employing spatial cross validation in order to eliminate the effects of spatial autocorrelation.

```{r}
#Calen working here
## First I downloaded all the layers, thanks Clara then brought them into my environment 
aspect<- raster("ta/ta/aspect.tif")
aspect <- rast(aspect)
slope <- raster("ta/ta/slope.tif")
slope <- rast(slope)
mrvbf <- raster("ta/ta//mrvbf.tif")
mrvbf <- rast(mrvbf)
mrrtf<- raster("ta/ta/mrrtf.tif")
mrrtf <- rast (mrrtf)
tri<- raster("ta/ta/tri.tif")
tri <- rast(tri)
twi <- raster("ta/ta/twi.tif")
twi <- rast (twi)
overlandflow_distance <- raster("ta/ta/o_flow.tif")
overlandflow_distance <- rast (overlandflow_distance)
openness_p <- raster("ta/ta/openness_pos.tif")
openness_p <- rast(openness_p)
openness_n <- raster("ta/ta/openness_neg.tif")
openness_n <- rast(openness_n)

#Reran Coopers part here just with new variables 
model_layers <- c(slope, aspect, mrvbf, mrrtf, tri, twi, overlandflow_distance, openness_p, openness_n)


#creating task function
str(ldb_data)
ldb_data$presence <- factor(ldb_data$presence)
tsk_grouse <- as_task_classif_st(ldb_data, target = "presence")
tsk_grouse

#creating probability learner
lrn_rf_tune_prob <- lrn("classif.ranger", 
                        num.trees = to_tune(100, 2000), predict_type = "prob",
                        mtry = to_tune(1, length(tsk_grouse$feature_names)),
                        importance = "impurity")

useful_msrs <- c("classif.logloss", "classif.mbrier")


#Tuner design

df_design <- expand.grid(
  num.trees = c(100, 250, 500, 750, 1000, 1500, 2000),
  mtry = 1:length(tsk_grouse$feature_names))

# Convert to a data.table (same structure as df_design, different object type)
dt_design <- data.table(df_design)

# Create the tuner object
tnr_design <- tnr("design_points", design = dt_design)

#Creating cross validation resampling objects 
cv_inner <- rsmp("cv", folds = 10)
cv_outer <- rsmp("spcv_coords", folds = 4)


#Creating auto tuner object

at_prob <- auto_tuner(
  tuner = tnr_design,
  learner = lrn_rf_tune_prob,
  resampling = cv_inner,
  measure = msr("classif.logloss"),
  terminator = trm("none")
)

#resampling - this takes a long time because i did not set up parallel env.
rr_prob <- resample(tsk_grouse, at_prob, cv_outer, store_models = TRUE)

rr_prob$aggregate(msrs(useful_msrs))

conf_prob <- rr_prob$prediction()$confusion

rr_prob_results <- extract_inner_tuning_results(rr_prob)

mod_scores_prob <- rr_prob$score(msrs(useful_msrs))
View(mod_scores_prob)

#best model
best_lrn_prob <- rr_prob$learners[[which.min(mod_scores_prob$classif.logloss)]]$learner
best_lrn_prob

imp <- data.frame(Variable = factor(names(imp), levels = rev(unique(names(imp)))),
                  Importance = imp, row.names = NULL)

imp_plot_prob <- ggplot(imp, aes(x = Importance, y = Variable)) + 
  geom_bar(stat = "identity")

imp_plot_prob

# Write variable importance to .csv file:
write.ftable(ftable(conf_prob), file = "confusion_matrix_prob.csv", sep = ",",
             quote = FALSE)

# Save that plot to a file (for use in reporting)
ggsave("Variable importance_prob.png", imp_plot_prob, width = 1920, height = 1440,
       units = "px", dpi = 300)


#The final plot!!!

ranger_model <- best_lrn_prob$model
fun <- function(model, ...) predict(model, ...)$predictions
prediction_prob_terra <- terra::predict(
  model_layers, ranger_model, fun = fun, na.rm = TRUE)

plot(prediction_prob_terra)
```
